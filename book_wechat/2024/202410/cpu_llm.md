# 能在 CPU 上运行的开源大模型推理框架



如今，大模型的发展势头迅猛。最初，大模型的演进出现了两种分化趋势：一方面，开源大模型的规模不断扩大，从最初的数十亿参数迅速扩展到数千亿参数，未来甚至可能突破万亿级的超大规模，这一趋势将大模型推向了计算能力的极限。这类巨型模型通过引入更丰富的语义理解、复杂的推理能力和跨领域知识整合，展现出强大的通用性能，尤其在生成、问答、翻译等复杂任务中具有显著的优势。这一方向的推动力来自各行业的广泛需求和科研领域的深度探索，为了追求更高的准确率和更强的泛化能力，大模型的“无上限”扩展不断突破技术和硬件的边界，也对计算资源、数据存储、训练时间提出了极高要求。

另一方面，模型也在向小型化方向发展，这主要是为了满足设备端的应用需求。用户端设备种类繁多，从配备强大计算能力的高端 PC 到资源受限的物联网设备，再到便携式智能设备和嵌入式系统，算力差异巨大。传统的大模型虽然功能强大，但无法在这些设备上高效运行，因此针对用户端的小型化模型应运而生。小型化模型力求在有限的资源环境中，实现较高的性能表现，以支持用户端的离线推理、实时响应以及低功耗应用。通过参数剪枝、知识蒸馏、量化等技术手段，小型模型可以有效减小体积和计算量，从而适应各类用户设备的运行条件，这对诸如自动驾驶、智能家居、可穿戴设备等场景尤为重要。

这种“大模型”与“小模型”的双轨发展，使得AI模型能够同时满足超大规模和轻量级设备的需求，实现了“集中式智能”与“边缘智能”的优势互补。这不仅丰富了人工智能的应用场景，也推动了大模型与小模型在不同环境中的深度融合与共生发展。

同时，模型量化技术的进步也为模型小型化提供了关键支撑。量化技术的原理类似于 MP3 的有损压缩：虽然会带来一定的质量损失，但对用户体验几乎没有影响。MP3 之所以比无损音频格式更受欢迎，正是因为在不显著牺牲音质的前提下，能极大缩小文件体积。量化技术在 AI 模型上也有类似效果，它通过减少数值表示的精度，显著缩减了模型的存储和计算量，使其更适合在低算力设备上运行。常见的量化方法包括将模型参数从原始的 32-bit 浮点数精度压缩为 16-bit、8-bit，甚至 4-bit，这样不仅减少了模型的内存占用，还能大幅降低推理时间，显著提升算力的利用效率。

如今，微软推出的 1-bit 量化技术则进一步将这一进程推向新高度，开辟了模型量化的新可能。1-bit 量化的核心在于仅保留模型权重的方向信息而舍弃其大小信息，极大地降低了数据传输和计算的复杂性。这种超低精度的量化技术虽然比传统量化方法更具挑战，但在特定的推理任务上已取得惊人的效果，为模型在低功耗设备和实时性要求高的场景中应用提供了新的可能。例如，1-bit 量化可以大幅加快模型推理速度，同时减少内存使用，使得智能设备可以在算力有限的情况下运行更复杂的 AI 功能，甚至可能在未来实现高效的端侧模型部署，从而推动智能设备的普及与应用场景的扩展。

这一突破不仅加速了模型小型化的步伐，也启发了在模型架构和训练方法上的进一步创新。随着量化技术的持续进步，从 8-bit 到 4-bit，再到如今的 1-bit，未来可能会有更多适应不同硬件环境的量化方案出现，从而加速模型应用的多样化。通过这些技术手段，AI 不仅能够更好地融入用户生活，还为边缘计算、物联网等领域提供了广阔的发展前景。

微软的 1-bit LLM（大语言模型）推理框架采用 “1-bit” 的名称，是因为它使用了 1-bit 量化技术，也称为 1-bit 梯度压缩。这种方法可以显著减少模型在推理和训练时的内存占用和计算成本。具体来说，1-bit 量化将模型权重或激活函数值量化到仅使用 1 位来表示，从而极大地压缩了数据体积。这种方法对大规模模型尤其有效，主要有以下优势：

* 内存效率：将权重压缩为 1-bit 表示后，存储需求大幅降低，使得模型能够在较小的内存环境中运行，降低推理设备的内存门槛。
* 计算加速：量化减少了数据的位数，计算密集型操作可以更快速完成，提高了推理速度。
* 带宽需求降低：对于分布式系统或多 GPU 训练场景，1-bit 量化减少了节点间的数据通信需求，加快了多设备协作速度。

尽管 1-bit 量化会牺牲一些精度，但对于推理任务，尤其是容错性较高的任务，这种方法能够在精度和效率之间取得较好的平衡。这也是 1-bit LLM 框架适合大规模模型的原因之一。

近日，微软发布了一个全新的开源项目——BitNet.cpp，这是专为1-bit大语言模型（LLMs）推理而设计的框架。BitNet.cpp旨在通过优化内核为CPU上运行的1.58-bit模型提供快速且无损的推理支持，并在未来版本中计划支持NPU和GPU。

BitNet.cpp的开源为1-bit LLM的普及和大规模推理打开了新的大门，其在CPU上的高效推理性能，极大地扩展了大模型在本地设备上的可行性。未来，随着对NPU和GPU的支持，BitNet.cpp有望成为低比特模型推理的主流框架。如果你对大模型在实际应用中的推理性能感兴趣，BitNet.cpp无疑是值得关注和尝试的项目。

下面介绍以下在 deepin v23 上如何使用 BitNet.cpp。

环境要求：

* Python 3.9及以上版本
* CMake 3.22及以上版本
* Clang 18及以上版本

## 检查 clang 版本并安装 clang 18

```bash
$ clang --version

Deepin clang version 17.0.6 (5deepin4)

Target: x86\_64-pc-linux-gnu

Thread model: posix

InstalledDir: /usr/bin
```

当前版本版本是 clang 17，首先得升级到 clang 18

```
$ sudo apt update
$ sudo apt install clang-18
$ sudo ln -sf ../lib/llvm-18/bin/clang /usr/bin/clang
```

安装后检查 clang 版本：

```
$ clang --version

Deepin clang version 18.1.7 (1)

Target: x86\_64-pc-linux-gnu

Thread model: posix

InstalledDir: /usr/bin
```

创建 python 环境

```
$ conda create -n bitnet-cpp python=3.9
$ conda activate bitnet-cpp
```


下载源码：

```
$ git clone --recursive https://github.com/microsoft/BitNet.git
$ pip install -r requirements.txt
$ python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s
INFO:root:Compiling the code using CMake.
INFO:root:Downloading model HF1BitLLM/Llama3-8B-1.58-100B-tokens from HuggingFace to models/Llama3-8B-1.58-100B-tokens...
INFO:root:Converting HF model to GGUF format...
INFO:root:GGUF model saved at models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf

```


**\$ python** utils/e2e\_benchmark.py **-m** models/Llama3-8B-1.58-100B-tokens/ggml-model-i2\_s.gguf **-n**200**-p**256**-t**4

| model                          |       size |     params | backend    | threads | n\_batch |          test |                  t/s |

| **--**--**--**--**--**--**--**--**--**--**--**--**--**--**--** | **--**--**--**--**-**: | **--**--**--**--**-**: | **--**--**--**--**--** | **--**--**--**: | **--**--**--**: | **--**--**--**--**--**--: | **--**--**--**--**--**--**--**--**--**-: |

| llama 8B I2\_S **-**2 bpw ternary  |   **3**.58 GiB |     **8**.03 B | CPU        |       **4** |       **1** |         pp256 |          **9**.11 ± **0**.06 |

| llama 8B I2\_S **-**2 bpw ternary  |   **3**.58 GiB |     **8**.03 B | CPU        |       **4** |       **1** |         tg200 |          **9**.09 ± **0**.02 |

build: 406a5036 (3947)

若有收获，就点个赞吧
