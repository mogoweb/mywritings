# 前言

我们最初在1979年末开始关注现在所谓的强化学习。我们俩都在马萨诸塞大学工作，致力于最早的项目之一，以复兴神经元样自适应元件网络可能被证明是一种有前途的方法人工智能。该项目探索了由A. Harry Klopf开发的“自适应系统的静力学理论”。 Harry的工作是丰富的思想源泉，我们被允许对其进行批判性的探索，并将其与自适应系统中以前的悠久历史进行比较。我们的任务变成了将想法分开并理解它们之间的关系和相对重要性之一。这种情况一直持续到今天，但是在1979年，我们意识到，也许最简单的想法（长期以来被认为是理所当然的）从计算的角度来看却很少受到关注。这仅仅是一个学习系统的想法，它需要某种东西，该东西可以适应其行为，以便最大化来自其环境的特殊信号。这就是“享乐主义”学习系统的想法，或者正如我们现在所说的，强化学习的想法。

像其他人一样，我们感到在控制论和人工智能的早期，强化学习已经得到了彻底的探索。但是，在仔细检查后，我们发现仅对其进行了少量探索。尽管强化学习显然激发了一些最早的学习计算研究的动机，但这些研究人员中的大多数都进行了其他事情，例如模式分类，监督学习和自适应控制，或者他们完全放弃了学习学习。结果，学习如何从环境中获取东西的特殊问题很少受到关注。回顾过去，专注于这一想法是使这一研究活动付诸实践的关键步骤。在增强学习的计算研究中几乎没有进展，直到人们认识到尚未彻底探索这样的基本思想。

从那时起，该领域已经走了很长一段路，并且在多个方向上不断发展和成熟。强化学习已逐渐成为机器学习，人工智能和神经网络研究中最活跃的研究领域之一。该领域已经建立了强大的数学基础和令人印象深刻的应用。强化学习的计算研究现已成为一个广阔的领域，全球有数百名活跃在不同领域的研究人员，例如心理学，控制理论，人工智能和神经科学。特别重要的是建立和发展了与最佳控制和动态规划理论之间关系的贡献。从互动中学习以实现目标的总体问题仍未解决，但我们对它的理解已大大改善。现在，我们可以将组件思想（例如时差学习，动态编程和函数逼近）放在针对整个问题的一致观点内。

我们写这本书的目的是提供清晰而简单的强化学习的关键思想和算法。我们希望所有相关学科的读者都可以使用我们的治疗方法，但是我们无法详细介绍所有这些观点。在大多数情况下，我们的处理都是从人工智能和工程学的角度出发。在第二版中，我们计划有一章概述与心理学和神经科学的联系，这些联系正在迅速发展。与其他领域的联系我们将留给他人或其他时间。我们还选择不对强化学习进行严格的正式处理。我们没有达到数学抽象的最高水平，也不依赖定理证明格式。我们试图选择一定程度的数学细节，以使数学朝正确的方向倾斜，而又不会分散基础思想的简单性和潜在的普遍性。

本书由三部分组成。第一部分是介绍性和面向问题的。我们专注于强化学习的最简单方面及其主要区别特征。一整章专门介绍了强化学习问题，我们将在本书其余部分中探讨其解决方案。第二部分介绍了所有基于估计动作值的基本求解方法的表格形式（假设有一个有限的状态空间）。我们介绍了动态编程，蒙特卡洛方法和时差学习。关于合格性跟踪的一章将后两种方法统一起来，而将规划方法（例如动态编程和状态空间搜索）和学习方法（例如蒙特卡洛和时差学习）统一起来的这一章。第三部分涉及扩展表格方法以包括各种形式的近似方法，包括函数近似方法，策略梯度方法以及为解决非政策学习问题而设计的方法。第四部分概述了生物学和应用中强化学习的一些前沿领域。

这本书的设计目的是在一两个学期的课程中用作课本，也许可以从文献中读到，也可以添加更数学的课本，例如Bertsekas和Tsitsiklis（1996）或Szepesvari（2010）。本书还可以用作机器学习，人工智能或神经网络的更广泛课程的一部分。在这种情况下，可能只希望覆盖一部分材料。我们建议您简要介绍第1章，第2章至第2.2节，第3.4章，第3.4、3.5和3.9节除外，然后根据时间和兴趣从其余各章中选择各节。第二部分的五个章节是相互依存的，最好按顺序进行介绍。其中，第6章对于本主题和本书的其余部分而言是最重要的。专注于机器学习或神经网络的课程应涵盖第9章，而专注于人工智能或规划的课程则应涵盖第8章。在整本书中，较困难且对本书其余部分而言并非必不可少的部分均标有“ ∗。可以在初读时忽略这些内容，以后再也不会造成问题。一些练习用*标记，表示它们是高级的，对于理解本章的基本材料而言不是必需的。

这本书基本上是独立的。 假定的唯一数学背景是熟悉概率的基本概念，例如对随机变量的期望。 如果读者具有一些人工神经网络知识或某种其他有监督的学习方法，则第9章将更容易理解，但无需先有背景即可阅读。 我们强烈建议您进行整本书中提供的练习。 解决方案手册可供教师使用。 这些以及其他相关及时的资料可通过Internet获得。

在大多数章节的末尾有一个名为“书目和历史评论”的部分，在此我们相信该章中提出的观点的来源，为进一步的阅读和进行中的研究提供指导，并描述相关的历史背景。 尽管我们试图使这些部分具有权威性和完整性，但毫无疑问，我们已经省略了一些重要的先前工作。 为此，我们深表歉意，并欢迎进行更正和扩展以将其合并到后续版本中。

从某种意义上说，我们致力于这本书已有三十年了，我们有很多人要感谢。首先，我们感谢那些亲自帮助我们发展本书中提出的总体观点的人：哈里·克洛普夫（Harry Klopf），帮助我们认识到需要重新开始强化学习。 Chris Watkins，Dimitri Bertsekas，John Tsitsiklis和Paul Werbos，帮助我们看到了关系对动态编程的价值； John Moore和Jim Kehoe，来自动物学习理论的见解和启发；奥立佛
塞尔弗里奇（Selfridge），强调适应的广度和重要性；更广泛地说，我们的同事和学生以无数的方式做出了贡献：罗恩·威廉姆斯，查尔斯·安德森，萨特·辛格，Sridhar Mahadevan，史蒂夫·布拉德克，鲍勃·克里特斯，彼得·达扬和里蒙·贝尔德。通过与Paul Cohen，Paul Utgoff，Martha Steenstrup，Gerry Tesauro，Mike Jordan，Leslie Kaelbling，Andrew Moore，Chris Atkeson，Tom Mitchell，Nils Nilsson，Stuart Russell，Tom Dietterich，Tom Dean的讨论，我们的强化学习观得到了极大的丰富。和Bob Narendra。我们感谢Michael Littman，Gerry Tesauro，Bob Crites，Satinder Singh和Wei Zhang分别提供了第4.7、15.1、15.4、15.5和15.6节的详细信息。我们感谢空军科学研究所，美国国家科学基金会和GTE实验室的长期和远见卓识的支持。

我们还要感谢阅读本书草稿并提供了宝贵意见的许多人，包括汤姆·卡尔特，约翰·齐齐克里斯，帕维尔·奇乔斯，奥勒·戈尔莫，查克·安德森，斯图尔特·罗素，本·范·罗伊，保罗·斯滕斯特鲁普，保罗·科恩 ，Sridhar Mahadevan，Jette Randlov，Brian Sheppard，Thomas O'Connell，Richard Coggins，Cristina Versino，John H.Hiett，Andreas Badelt，Jay Ponte，Joe Beck，Justus Piater，Martha Steenstrup，Satinder Singh，Tommi Jaakkola，Dimitri Bertsekas， 托比洪·埃克曼（Torbjorn Ekman），克里斯蒂娜·比约克曼（Christina Bjorkman），雅各布·卡尔斯特朗（JakobCarlstréom）和奥勒·帕姆格伦（Olle Palmgren）。 最后，我们感谢格温·米切尔（Gwyn Mitchell）在许多方面的帮助，以及哈里·斯坦顿（Harry Stanton）和鲍勃·普里尔（Bob Prior）在麻省理工学院出版社的获奖。